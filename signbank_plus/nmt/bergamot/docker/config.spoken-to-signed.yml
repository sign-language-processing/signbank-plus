experiment:
  name: spoken_to_signed_bpe4
  src: spoken
  trg: signed
  spm-vocab-size: 8000
  spm-user-defined-symbols: "$ca,$haf,$ssp,$cse,$isr,$ukl,$eo,$tss,$isg,$mw,$sfs,$nsp,$xml,$tsq,$svk,$uk,$jos,$esl,$es,$ugy,$pt,$tsm,$bfi,$tr,$fil,$hsh,$gr,$mfs,$sdl,$ms,$sk,$bvl,$bg,$en,$ru,$fr,$slf,$gsg,$no,$jsl,$vsl,$bzs,$zh-TW,$mt,$sq,$ht,$fsl,$xki,$ins,$vgt,$da,$eth,$ps,$zh-CN,$esn,$hi,$asq,$hds,$sl,$tse,$pys,$pl,$pks,$ssr,$gn,$mdl,$dse,$sls,$lws,$ne,$fse,$nzs,$he,$zh,$prl,$ncs,$de,$ja,$ko,$csg,$csl,$psr,$ar,$sw,$ysl,$pso,$sfb,$bqn,$fi,$cs,$nsi,$swl,$rsl,$vn,$nsl,$rms,$csn,$sv,$is,$sgg,$afg,$aed,$fcs,$th,$csc,$hu,$psc,$sqk,$,$icl,$am,$ur,$ase,$nl,$kvk,$it,$ro,$gss,$dsl,$asf,$ise,$psp"

  teacher-ensemble: 2
  # path to a pretrained backward model (optional)
  backward-model: ""
  # path to a pretrained vocabulary (optional)
  vocab: ""

  # limits per downloaded dataset
  mono-max-sentences-src: 100000000
  mono-max-sentences-trg: 20000000
  # split corpus to parallelize translation
  split-chunks: 10
  # vocab training sample
  spm-sample-size: 10000000

  best-model: chrf

  use-opuscleaner: false
  bicleaner:
    default-threshold: 0  # There is no bicleaner support for "spoken" and "signed". TODO: train such a cleaner?
    dataset-thresholds: {}


marian-args:
  training-backward:
    after: 30e

    valid-max-length: 512
    max-length: 512

  decoding-backward:
    # 12 Gb GPU, s2s model
    mini-batch-words: 2000
    beam-size: 12

    max-length: 512

  decoding-teacher:
    mini-batch-words: 1000

    max-length: 512

  training-teacher-base:
    valid-max-length: 512
    max-length: 512

  training-teacher-finetuned:
    valid-max-length: 512
    max-length: 512

  training-student:
    valid-max-length: 512
    max-length: 512

  training-student-finetuned:
    valid-max-length: 512
    max-length: 512

datasets:
  # parallel training corpus
  train:
    - custom-corpus_/corpora/parallel/cleaned/train
    - custom-corpus_/corpora/parallel/cleaned/dev
    - custom-corpus_/corpora/parallel/more/train
  devtest:
    - custom-corpus_/corpora/parallel/test/all
  test:
    - custom-corpus_/corpora/parallel/test/all
  # monolingual datasets (ex. paracrawl-mono_paracrawl8, commoncrawl_wmt16, news-crawl_news.2020)
  # to be translated by the teacher model
  mono-src:
    - custom-mono_/corpora/mono/words/mono
  # to be translated by the backward model to augment teacher corpus with back-translations
  # leave empty to skip augmentation step (high resource languages)
  mono-trg: [ ] # TODO, create monolingual signed language data

