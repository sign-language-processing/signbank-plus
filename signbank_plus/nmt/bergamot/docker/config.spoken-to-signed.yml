####
# Example of a production config
# Change language pair, experiment name, datasets and other settings if needed
# Training low resource languages might require more tuning of pipeline/training/configs
###


experiment:
  name: spoken_to_signed
  src: spoken
  trg: signed
  spm-vocab-size: 1668 # Words tokenizer can't run with number higher than the number of words

  teacher-ensemble: 2
  # path to a pretrained backward model (optional)
  backward-model: ""
  # path to a pretrained vocabulary (optional)
  vocab: ""

  # limits per downloaded dataset
  mono-max-sentences-src: 100000000
  mono-max-sentences-trg: 20000000
  # split corpus to parallelize translation
  split-length: 2000000
  # vocab training sample
  spm-sample-size: 10000000

  best-model: chrf

  use-opuscleaner: false
  bicleaner:
    default-threshold: 0  # There is no bicleaner support for "spoken" and "signed". TODO: train such a cleaner?
    dataset-thresholds: {}


marian-args:
  # these configs override pipeline/train/configs
  training-backward:
    # change based on available training data
    after: 20e
  #  training-teacher-base:
    # remove for low resource languages or if training without augmentation
    # after: 2e
  # these configs override pipeline/translate/decoder.yml
  decoding-backward:
    # 12 Gb GPU, s2s model
    mini-batch-words: 2000
    beam-size: 12
  decoding-teacher:
    # 12 Gb GPU, ensemble of 2 teachers
    mini-batch-words: 1000
    # 2080ti or newer
    # precision: float16

datasets:
  # parallel training corpus
  train:
    - custom-corpus_/corpora/parallel/cleaned/train
    - custom-corpus_/corpora/parallel/expanded/train
    - custom-corpus_/corpora/parallel/more/train
  devtest:
    - custom-corpus_/corpora/parallel/cleaned/dev
  test:
    - custom-corpus_/corpora/parallel/test/all
  # monolingual datasets (ex. paracrawl-mono_paracrawl8, commoncrawl_wmt16, news-crawl_news.2020)
  # to be translated by the teacher model
  mono-src:
    - custom-mono_/corpora/mono/words/mono
  # to be translated by the backward model to augment teacher corpus with back-translations
  # leave empty to skip augmentation step (high resource languages)
  mono-trg: [ ] # TODO, create monolingual signed language data

